
import os
import numpy as np
import json

def pack_weights_unpacked(weights, bits):
    """
    Stores weights as 1 byte per weight, but only uses the specified number of bits.
    This is 'Pseudo-Packed': we save compute cycles but not memory.
    To save memory, we would need to pack multiple weights per byte and unpack in HW.
    For Phase 1, we use this simple format to verify bit-serial logic.
    """
    # Mask to bit-width
    mask = (1 << bits) - 1
    weights_masked = weights.astype(np.int32) & mask
    return bytearray(weights_masked.astype(np.uint8).tolist())

def main():
    # Load HAWQ results
    json_path = "hawq_results.json"
    if not os.path.exists(json_path):
        json_path = "artifacts/hawq_results.json"
        
    # Default if missing
    allocation = {"dw": 8, "pw": 4}
    if os.path.exists(json_path):
        with open(json_path, "r") as f:
            config = json.load(f)
            if "allocation" in config:
                allocation = config["allocation"]
    
    # Load existing 8-bit bins (baseline)
    # We assume these were generated by export_bins.py or similar
    dw_bin_path = "artifacts/dw3x3_c3.bin"
    pw_bin_path = "artifacts/pw1x1_c10x3.bin"
    
    # Create dummy bins if missing (for testing flow)
    if not os.path.exists(dw_bin_path):
        print(f"Warning: {dw_bin_path} not found. Creating dummy random weights.")
        os.makedirs("artifacts", exist_ok=True)
        # DW: 3 channels * 3*3 kernel = 27 bytes
        np.random.randint(0, 255, 27, dtype=np.uint8).tofile(dw_bin_path)
    
    if not os.path.exists(pw_bin_path):
        print(f"Warning: {pw_bin_path} not found. Creating dummy random weights.")
        # PW: 10 out * 3 in = 30 bytes
        np.random.randint(0, 255, 30, dtype=np.uint8).tofile(pw_bin_path)

    dw_weights = np.fromfile(dw_bin_path, dtype=np.uint8)
    pw_weights = np.fromfile(pw_bin_path, dtype=np.uint8)
    
    # Repack DW
    # Check if keys are 'dw' or 'dw_layer' based on hawq_results.json
    dw_bits = allocation.get("dw_layer", allocation.get("dw", 8))
    
    # Re-quantize logic (Placeholder: just mask assuming already quantized range)
    # In real flow, we would re-scale. Here we just take MSBs or mask.
    # To properly emulate "low precision", we should quantization-aware train or post-training quant.
    # Here: Simple Masking of existing 8-bit weights (Valid for testing pipe)
    dw_packed = pack_weights_unpacked(dw_weights, dw_bits)
    
    # Repack PW
    pw_bits = allocation.get("pw_layer", allocation.get("pw", 4))
    pw_packed = pack_weights_unpacked(pw_weights, pw_bits)
    
    # Save mixed bins
    os.makedirs("artifacts/mixed", exist_ok=True)
    with open("artifacts/mixed/dw_mixed.bin", "wb") as f: f.write(dw_packed)
    with open("artifacts/mixed/pw_mixed.bin", "wb") as f: f.write(pw_packed)
    
    # Save Metadata for HLS/Firmware
    # Format: [Layer ID, Bits, Offset, Size]
    # We use size in BYTES (which is same as Count in this unpacked scheme)
    metadata = np.array([
        0, dw_bits, 0, len(dw_packed),
        1, pw_bits, 0, len(pw_packed)
    ], dtype=np.int32)
    metadata.tofile("artifacts/mixed/metadata.bin")
    
    print(f"Repacking complete (Unpacked Storage):")
    print(f"  DW: {dw_bits}-bit, size: {len(dw_weights)} bytes")
    print(f"  PW: {pw_bits}-bit, size: {len(pw_weights)} bytes")
    print(f"Saved to artifacts/mixed/")

if __name__ == "__main__":
    main()
